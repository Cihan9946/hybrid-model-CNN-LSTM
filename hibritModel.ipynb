{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41vRzXbwelK_"
      },
      "source": [
        "XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KlC6gJIFWxe",
        "outputId": "728fd475-140f-4cd2-f701-710f57e274da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-440893d92c30>:7: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  store_sales = pd.read_csv(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data\n",
        "comp_dir = Path('/content/drive/MyDrive/HibritModel')\n",
        "\n",
        "store_sales = pd.read_csv(\n",
        "    comp_dir / 'train.csv',\n",
        "    usecols=['store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n",
        "    dtype={\n",
        "        'store_nbr': 'category',\n",
        "        'family': 'category',\n",
        "        'sales': 'float32',\n",
        "        'onpromotion': 'float32',  # Değişiklik burada\n",
        "    },\n",
        "    parse_dates=['date'],\n",
        "    infer_datetime_format=True,\n",
        ")\n",
        "store_sales['date'] = store_sales.date.dt.to_period('D')\n",
        "store_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n",
        "\n",
        "# Verisetinden 330,000 satırdan sonrasını silmek\n",
        "store_sales = store_sales.iloc[:500000]\n",
        "\n",
        "average_sales = (\n",
        "    store_sales\n",
        "    .groupby('date').mean()\n",
        "    .squeeze()\n",
        "    .loc['2017']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnbhOpHenKzG"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
        "\n",
        "#define target\n",
        "y = store_sales.unstack(['store_nbr', 'family']).loc[\"2017\"]\n",
        "\n",
        "# Create training data\n",
        "fourier = CalendarFourier(freq='M', order=4)\n",
        "dp = DeterministicProcess(\n",
        "    index=y.index,\n",
        "    constant=True,\n",
        "    order=1,\n",
        "    seasonal=True,\n",
        "    additional_terms=[fourier],\n",
        "    drop=True,\n",
        ")\n",
        "X = dp.in_sample()\n",
        "X['NewYear'] = (X.index.dayofyear == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvuoxMNHnkOm",
        "outputId": "9c584c6f-f1d9-407a-f23c-f3e2cb6e8810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.2)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install statsmodels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-dE9VD8DlWn",
        "outputId": "80404289-d454-43d9-be02-0891d09fa740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x2klVbKwnoGQ"
      },
      "outputs": [],
      "source": [
        "# Gerekli kütüphaneleri içe aktarın\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from xgboost import XGBRegressor\n",
        "y = y.fillna(y.mean())\n",
        "# Modeli tanımlayın ve eğitin\n",
        "model = RegressorChain(base_estimator=XGBRegressor())\n",
        "model.fit(X, y)\n",
        "\n",
        "# Tahminleri yapın\n",
        "y_pred = pd.DataFrame(model.predict(X), index=X.index, columns=y.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vyD65PMfpqbp",
        "outputId": "b688fa10-61cc-41f2-a895-72f9b346ec2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-d8546ba37674>:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  test_df = pd.read_csv(\n"
          ]
        }
      ],
      "source": [
        "#Load test data\n",
        "test_df = pd.read_csv(\n",
        "    comp_dir / 'test.csv',\n",
        "    dtype={\n",
        "        'store_nbr': 'category',\n",
        "        'family': 'category',\n",
        "        'onpromotion': 'uint32',\n",
        "    },\n",
        "    parse_dates=['date'],\n",
        "    infer_datetime_format=True,\n",
        ")\n",
        "test_df['date'] = test_df.date.dt.to_period('D')\n",
        "test_df = test_df.set_index(['store_nbr', 'family', 'date']).sort_index()\n",
        "\n",
        "# Create features for test set\n",
        "X_test = dp.out_of_sample(steps=16)\n",
        "X_test.index.name = 'date'\n",
        "X_test['NewYear'] = (X_test.index.dayofyear == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsu1lH90puGQ"
      },
      "outputs": [],
      "source": [
        "y_submit = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\n",
        "y_submit = y_submit.stack(['store_nbr', 'family'])\n",
        "y_submit = y_submit.join(test_df.id).reindex(columns=['id', 'sales'])\n",
        "y_submit.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8wndrb7pw2M"
      },
      "source": [
        "EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm3yVkMSpwiH"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EDiFWj8p0sE"
      },
      "outputs": [],
      "source": [
        "train['date'] = pd.to_datetime(train['date'])\n",
        "train.rename(columns={'sales': 'Sales'}, inplace=True)\n",
        "train['Year'] = train['date'].dt.year\n",
        "train['Month'] = train['date'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXg1RxA1qFP3"
      },
      "outputs": [],
      "source": [
        "# Inspecting time series and rolling mean:\n",
        "\n",
        "crossing = train[['date', 'Sales']].groupby('date').sum()\n",
        "tseries = train.groupby(['date'])['Sales'].agg(\n",
        "    ['sum']).reset_index().rename(columns={'sum': 'Sales'})\n",
        "tseries = tseries.set_index('date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbaxCWNbqG_x"
      },
      "outputs": [],
      "source": [
        "y = tseries['Sales'].resample('MS').mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceKjsk3OqMKm",
        "outputId": "48324bf4-5c3f-4160-8a0d-733278fc524a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Series([], Freq: MS, Name: Sales, dtype: float64)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y['2017':]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "9wpd8kNgqQgw",
        "outputId": "7ebb8842-c0f4-4708-ab69-8a004ab15c00",
        "collapsed": true
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "x must have 2 complete cycles requires 24 observations. x only has 7 observation(s)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-20ea552ff576>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdecomposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseasonal_decompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'additive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/seasonal.py\u001b[0m in \u001b[0;36mseasonal_decompose\u001b[0;34m(x, model, filt, period, two_sided, extrapolate_trend)\u001b[0m\n\u001b[1;32m    170\u001b[0m             )\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpfreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;34mf\"x must have 2 complete cycles requires {2 * pfreq} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;34mf\"observations. x only has {x.shape[0]} observation(s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x must have 2 complete cycles requires 24 observations. x only has 7 observation(s)"
          ]
        }
      ],
      "source": [
        "from pylab import rcParams\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "import statsmodels.api as sm\n",
        "rcParams['figure.figsize'] = 18, 8\n",
        "\n",
        "decomposition = sm.tsa.seasonal_decompose(y, model='additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAKpAlFlqTQu"
      },
      "source": [
        "ARIMA (Seasonal ARIMA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-MnCpKfqT_U"
      },
      "outputs": [],
      "source": [
        "# Based on a previous run, evaluation parameters of PDQ at (1,1,0) yielded lowest MSE\n",
        "p = d = q = range(0, 2)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E049qm3mqXUH"
      },
      "outputs": [],
      "source": [
        "mod = sm.tsa.statespace.SARIMAX(y,\n",
        "                                order=(1, 1, 1),\n",
        "                                seasonal_order=(1, 1, 0, 12),\n",
        "                                enforce_stationarity=False,\n",
        "                                enforce_invertibility=False)\n",
        "\n",
        "results = mod.fit()\n",
        "\n",
        "print(results.summary().tables[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQTBXwKUqcKL"
      },
      "outputs": [],
      "source": [
        "results.plot_diagnostics(figsize=(16, 8)) # Investigating the results:\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5PMvlC_qepv"
      },
      "outputs": [],
      "source": [
        "pred = results.get_prediction(start=pd.to_datetime('2017-04-01'), dynamic=False)\n",
        "pred_ci = pred.conf_int()\n",
        "\n",
        "ax = y['2017':].plot(label='observed')\n",
        "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\n",
        "\n",
        "ax.fill_between(pred_ci.index,\n",
        "                pred_ci.iloc[:, 0],\n",
        "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
        "\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Sales')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu4bXQNVqhBd"
      },
      "outputs": [],
      "source": [
        "pred_uc = results.get_forecast(steps=16)\n",
        "pred_ci = pred_uc.conf_int()\n",
        "\n",
        "ax = y.plot(label='observed', figsize=(14, 7))\n",
        "pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
        "ax.fill_between(pred_ci.index,\n",
        "                pred_ci.iloc[:, 0],\n",
        "                pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Sales')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTKWEpraqjh-"
      },
      "outputs": [],
      "source": [
        "y_forecasted = pred.predicted_mean\n",
        "y_truth = y['2017-01-01':]\n",
        "\n",
        "# Compute the mean square error\n",
        "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
        "print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFZhV4W5qsb1"
      },
      "source": [
        "Data Processing for Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzVhq9MTqnhS"
      },
      "outputs": [],
      "source": [
        "!pip install chart_studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_EDPtDLqp6B"
      },
      "outputs": [],
      "source": [
        "#Set up\n",
        "import warnings\n",
        "from keras import optimizers\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import plotly.graph_objs as go\n",
        "from chart_studio import plotly\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "init_notebook_mode(connected=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0EysBCFqubl"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df = pd.read_csv(('/content/train.csv'), parse_dates=['date'])\n",
        "test_df = pd.read_csv(('/content/test.csv'), parse_dates=['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gYZZrYBrzAb"
      },
      "outputs": [],
      "source": [
        "# Creating a instance of label Encoder.\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Using .fit_transform function to fit label\n",
        "# encoder and return encoded label\n",
        "label = le.fit_transform(train_df['family'])\n",
        "\n",
        "# printing label\n",
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnYQ4ErDr2hv"
      },
      "outputs": [],
      "source": [
        "label2 = le.fit_transform(train_df['store_nbr'])\n",
        "label2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJRloU9Vr6BI"
      },
      "outputs": [],
      "source": [
        "train_df.drop((\"family\"), axis=1, inplace=True)\n",
        "train_df.drop((\"store_nbr\"), axis=1, inplace=True)\n",
        "\n",
        "# Appending the array to dataFrame\n",
        "\n",
        "train_df[\"family\"] = label\n",
        "train_df[\"store_nbr\"] = label2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsdKu5okr8cB"
      },
      "outputs": [],
      "source": [
        "print('Min date from train set: %s' % train_df['date'].min().date())\n",
        "print('Max date from train set: %s' % train_df['date'].max().date())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWawKBTir-MT"
      },
      "outputs": [],
      "source": [
        "lag_size = (test_df['date'].max().date() - train_df['date'].max().date()).days\n",
        "print('Max date from train set: %s' % train_df['date'].max().date())\n",
        "print('Max date from test set: %s' % test_df['date'].max().date())\n",
        "print('Forecast lag size', lag_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIabPiLMsCMp"
      },
      "source": [
        "Transform data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcyaAj7vsDTh"
      },
      "outputs": [],
      "source": [
        "train_df = train_df[(train_df['date'] >= '2017-01-01')] #working on data starting from this date\n",
        "#Rearrange data\n",
        "train_gp = train_df.sort_values('date').groupby(['family', 'store_nbr', 'date','onpromotion'], as_index=False)\n",
        "train_gp = train_gp.agg({'sales':['mean']})\n",
        "train_gp.columns = ['family', 'store_nbr', 'date', 'onpromotion','sales']\n",
        "train_gp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpK_SFcisF7b"
      },
      "outputs": [],
      "source": [
        "# Transform series to supervised learning\n",
        "\n",
        "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
        "    cols, names = list(), list()\n",
        "    # Input sequence (t-n, ... t-1)\n",
        "    for i in range(window, 0, -1):\n",
        "        cols.append(data.shift(i))\n",
        "        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
        "    # Current timestep (t=0)\n",
        "    cols.append(data)\n",
        "    names += [('%s(t)' % (col)) for col in data.columns]\n",
        "    # Target timestep (t=lag)\n",
        "    cols.append(data.shift(-lag))\n",
        "    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
        "    # Put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # Drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "\n",
        "#use the current timestep and the last 29 to forecast 16 days ahead\n",
        "window = 29\n",
        "lag = lag_size\n",
        "series = series_to_supervised(train_gp.drop('date', axis=1), window=window, lag=lag)\n",
        "series.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxMtyEEnsKvf"
      },
      "outputs": [],
      "source": [
        "#Drop rows with different item or store values than the shifted columns\n",
        "\n",
        "last_item = 'family(t-%d)' % window\n",
        "last_store = 'store_nbr(t-%d)' % window\n",
        "last_onpromotion = 'onpromotion(t-%d)' % window\n",
        "series = series[(series['store_nbr(t)'] == series[last_store])]\n",
        "series = series[(series['family(t)'] == series[last_item])]\n",
        "series = series[(series['onpromotion(t)'] == series[last_item])]\n",
        "#Remove unwanted columns\n",
        "columns_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['family', 'store_nbr','onpromotion']]\n",
        "for i in range(window, 0, -1):\n",
        "    columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['family', 'store_nbr','onpromotion']]\n",
        "series.drop(columns_to_drop, axis=1, inplace=True)\n",
        "series.drop(['family(t)', 'store_nbr(t)'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw7_AXWgsOyG"
      },
      "outputs": [],
      "source": [
        "# Label\n",
        "labels_col = 'sales(t+%d)' % lag_size\n",
        "labels = series[labels_col]\n",
        "series = series.drop(labels_col, axis=1)\n",
        "\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(series, labels.values, test_size=0.2, random_state=0)\n",
        "print('Train set shape', X_train.shape)\n",
        "print('Validation set shape', X_valid.shape)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Veq-gU83sTNk"
      },
      "source": [
        "CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbzf3UYUsRMM"
      },
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "epochs = 1000\n",
        "batch = 128\n",
        "lr = 0.00001\n",
        "adam = optimizers.Adam(lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l89vAvfmsWQk"
      },
      "outputs": [],
      "source": [
        "X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
        "print('Train set shape', X_train_series.shape)\n",
        "print('Validation set shape', X_valid_series.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPfkGoehsYSe"
      },
      "outputs": [],
      "source": [
        "cnn = Sequential()\n",
        "cnn.add(Conv1D(filters=128, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
        "cnn.add(MaxPooling1D(pool_size=2))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense (128, activation='relu'))\n",
        "cnn.add(Dropout(0.2))\n",
        "cnn.add(Dense (128, activation='relu'))\n",
        "cnn.add(Dropout(0.2))\n",
        "cnn.add(Dense (34, activation='relu'))\n",
        "cnn.add(Dropout(0.2))\n",
        "cnn.add(Dense(1))\n",
        "cnn.compile(loss='mse', optimizer=adam)\n",
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNe4ZEmusaS_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "cnn_history = cnn.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), callbacks=[callback],epochs=epochs, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL5HEHwxskih"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoPNywwGsk_A"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "batch = 128\n",
        "lr = 0.00001\n",
        "adam = optimizers.Adam(lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR7ewbiZsoBm"
      },
      "outputs": [],
      "source": [
        "lstm = Sequential()\n",
        "lstm.add(LSTM(256, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
        "lstm.add(Dense(128))\n",
        "lstm.add(Dropout(0.2))\n",
        "lstm.add(Dense(64))\n",
        "lstm.add(Dense(32))\n",
        "lstm.add(Dense(1))\n",
        "lstm.compile(loss='mse', optimizer=adam)\n",
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V82zdc_qsp-h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "lstm_history = lstm.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid),callbacks=[callback], epochs=epochs, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mgUe3FXsupT"
      },
      "source": [
        "CNN-LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8j2jtdrss8w"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "batch = 128\n",
        "lr = 0.00001\n",
        "adam = optimizers.Adam(lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PidPy1nTsxNt"
      },
      "outputs": [],
      "source": [
        "subsequences = 1\n",
        "timesteps = X_train_series.shape[1]//subsequences\n",
        "X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, timesteps, 1))\n",
        "X_valid_series_sub = X_valid_series.reshape((X_valid_series.shape[0], subsequences, timesteps, 1))\n",
        "print('Train set shape', X_train_series_sub.shape)\n",
        "print('Validation set shape', X_valid_series_sub.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GrJB4eJs1fd"
      },
      "outputs": [],
      "source": [
        "cnn_lstm = Sequential()\n",
        "cnn_lstm.add(TimeDistributed(Conv1D(filters=128, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
        "cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "cnn_lstm.add(TimeDistributed(Flatten()))\n",
        "cnn_lstm.add(TimeDistributed(Dropout(0.2)))\n",
        "cnn_lstm.add(LSTM(64, activation='relu'))\n",
        "cnn_lstm.add(Dense(32))\n",
        "cnn_lstm.add(Dense(1))\n",
        "cnn_lstm.compile(loss='mse', optimizer=adam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8ue3XvPs3aw"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N00PTKNys41d"
      },
      "outputs": [],
      "source": [
        "cnn_lstm_history = cnn_lstm.fit(X_train_series_sub, Y_train, validation_data=(X_valid_series_sub, Y_valid),callbacks=[callback], epochs=epochs, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niuc3Shas-Di"
      },
      "source": [
        "Evaluate Learning Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZF1v4RFtA6Z"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 1, sharey=False,figsize=(22,20))\n",
        "ax1 = axes[0]\n",
        "ax2 = axes[1]\n",
        "ax3 = axes[2]\n",
        "\n",
        "\n",
        "ax1.plot(cnn_history.history['loss'], label='Train loss')\n",
        "ax1.plot(cnn_history.history['val_loss'], label='Validation loss')\n",
        "ax1.legend(loc='best')\n",
        "ax1.set_title('CNN MODEL')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('MSE')\n",
        "\n",
        "ax2.plot(lstm_history.history['loss'], label='Train loss')\n",
        "ax2.plot(lstm_history.history['val_loss'], label='Validation loss')\n",
        "ax2.legend(loc='best')\n",
        "ax2.set_title('LSTM MODEL')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('MSE')\n",
        "\n",
        "ax3.plot(cnn_lstm_history.history['loss'], label='Train loss')\n",
        "ax3.plot(cnn_lstm_history.history['val_loss'], label='Validation loss')\n",
        "ax3.legend(loc='best')\n",
        "ax3.set_title('CNN-LSTM MODEL')\n",
        "ax3.set_xlabel('Epochs')\n",
        "ax3.set_ylabel('MSE')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pDc4RG5tHAy"
      },
      "source": [
        "Evaluate RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blnDyEHPtE1X"
      },
      "outputs": [],
      "source": [
        "cnn_train_pred = cnn.predict(X_train_series)\n",
        "cnn_valid_pred = cnn.predict(X_valid_series)\n",
        "print('Train CNN rmse:', np.sqrt(mean_squared_error(Y_train, cnn_train_pred)))\n",
        "print('Validation CNN rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l5tqpFitLQA"
      },
      "outputs": [],
      "source": [
        "lstm_train_pred = lstm.predict(X_train_series)\n",
        "lstm_valid_pred = lstm.predict(X_valid_series)\n",
        "print('Train LSTM rmse:', np.sqrt(mean_squared_error(Y_train, lstm_train_pred)))\n",
        "print('Validation LSTM rmse:', np.sqrt(mean_squared_error(Y_valid, lstm_valid_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icW0G4KktNOe"
      },
      "outputs": [],
      "source": [
        "cnn_lstm_train_pred = cnn_lstm.predict(X_train_series_sub)\n",
        "cnn_lstm_valid_pred = cnn_lstm.predict(X_valid_series_sub)\n",
        "print('Train CNN-LSTM rmse:', np.sqrt(mean_squared_error(Y_train, cnn_lstm_train_pred)))\n",
        "print('Validation CNN-LSTM rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_lstm_valid_pred)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}